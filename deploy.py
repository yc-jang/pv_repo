# report_models.ipynb / (ë˜ëŠ” .py) ì…€ì— ë¶™ì—¬ë„£ì–´ ì‚¬ìš©
from __future__ import annotations

from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from IPython.display import display, Markdown

# í™˜ê²½ì— ë§ê²Œ ì¡°ì •
from model_cat import load_and_inspect   # v1 (runner v5 ê¸°ë°˜)
from runner import CatBoostRunner        # v5

# =========================
# Config
# =========================
DEPLOY_DIR = Path("models/deploy")
MODEL_EXTS = (".pkl", ".joblib")  # í•„ìš”ì‹œ í™•ì¥ì ì¶”ê°€
TOPK = 20                          # shap_summary_bar / waterfall ê³µí†µ topk


# =========================
# Utilities
# =========================
def find_models(deploy_dir: Path, exts: Tuple[str, ...]) -> List[Path]:
    """Find model artifact files under deploy_dir."""
    paths: List[Path] = []
    if not deploy_dir.exists():
        raise FileNotFoundError(f"Deploy directory not found: {deploy_dir}")
    for p in deploy_dir.rglob("*"):
        if p.is_file() and p.suffix.lower() in exts:
            paths.append(p)
    if not paths:
        raise FileNotFoundError(f"No model files with {exts} under {deploy_dir}")
    return sorted(paths)


def load_metadata_csv(deploy_dir: Path) -> Optional[pd.DataFrame]:
    """Load optional metadata.csv to enrich 'í•™ìŠµ íŒŒì¼ ì •ë³´'."""
    meta_path = deploy_dir / "metadata.csv"
    if meta_path.exists():
        try:
            df = pd.read_csv(meta_path)
            # ê¶Œì¥ ì»¬ëŸ¼ ì˜ˆì‹œ: filename, train_data_path, created_by, created_at, notes...
            return df
        except Exception:
            return None
    return None


def md_title(txt: str, level: int = 2) -> None:
    """Display a Markdown title."""
    display(Markdown("#" * level + f" {txt}"))


def safe_preview(df: Optional[pd.DataFrame], n: int = 5) -> Optional[pd.DataFrame]:
    """Return a small preview of a DataFrame (or None)."""
    if df is None:
        return None
    try:
        return df.head(n)
    except Exception:
        return None


# =========================
# Section Renderers
# =========================
def render_training_file_info(runner: CatBoostRunner,
                              bundle_summary: Dict[str, Any],
                              meta_row: Optional[pd.Series]) -> None:
    """í•™ìŠµ íŒŒì¼/í™˜ê²½ ê´€ë ¨ ì •ë³´ ì„¹ì…˜."""
    md_title("1) í•™ìŠµì— ì‚¬ìš©ëœ íŒŒì¼/í™˜ê²½ ì •ë³´", level=3)

    rows = []

    # metadata.csv ìš°ì„ 
    if meta_row is not None:
        for k, v in meta_row.items():
            rows.append(("metadata." + str(k), v))

    # runner/bundleì—ì„œ ìœ ì˜ë¯¸í•œ ê²ƒë“¤
    rows.extend([
        ("model_name", bundle_summary.get("model")),
        ("timestamp", bundle_summary.get("timestamp", "N/A")),
        ("columns(count)", len(bundle_summary.get("columns") or [])),
        ("cat_features", bundle_summary.get("cat_features")),
        ("ntree_end", bundle_summary.get("ntree_end")),
        ("params(best)", runner.best_params),
    ])

    info_df = pd.DataFrame(rows, columns=["key", "value"])
    display(info_df)


def render_test_info(runner: CatBoostRunner, bundle_summary: Dict[str, Any]) -> None:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´ ì„¹ì…˜."""
    md_title("2) í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´", level=3)

    x_shape = bundle_summary.get("X_test_shape")
    y_len = bundle_summary.get("y_test_len")
    rows = [
        ("X_test_shape", x_shape),
        ("y_test_len", y_len),
    ]
    info_df = pd.DataFrame(rows, columns=["key", "value"])
    display(info_df)

    # ë¯¸ë¦¬ë³´ê¸°(ê°€ëŠ¥í•œ ê²½ìš°)
    x_prev = safe_preview(runner.X_test, n=5)
    y_prev = safe_preview(runner.y_test.to_frame(name="y_test") if runner.y_test is not None else None, n=5)

    if x_prev is not None:
        display(Markdown("**X_test preview (head)**"))
        display(x_prev)
    if y_prev is not None:
        display(Markdown("**y_test preview (head)**"))
        display(y_prev)


def render_training_results(runner: CatBoostRunner,
                            scores: Dict[str, float],
                            overfit_report_df: Optional[pd.DataFrame],
                            figures: Dict[str, Any]) -> None:
    """í•™ìŠµ ê²°ê³¼ ì„¹ì…˜: ì ìˆ˜/ì˜¤ë²„í•/ëŸ¬ë‹ì»¤ë¸Œ/ì˜ˆì¸¡-ì •ë‹µ."""
    md_title("3) í•™ìŠµ ê²°ê³¼", level=3)

    # ì ìˆ˜ í…Œì´ë¸”
    score_df = pd.DataFrame([scores])
    display(Markdown("**Scores (test)**"))
    display(score_df)

    # ì˜¤ë²„í”¼íŒ… ë¦¬í¬íŠ¸
    if overfit_report_df is not None:
        display(Markdown("**Overfitting Report**"))
        display(overfit_report_df)

    # ì˜ˆì¸¡ vs ì •ë‹µ / ì‚°ì ë„ / ì”ì°¨ / ëŸ¬ë‹ì»¤ë¸Œ
    if figures.get("line") is not None:
        display(Markdown("**ì˜ˆì¸¡ vs ì •ë‹µ (Line)**"))
        figures["line"].show()

    if figures.get("pred_vs_true") is not None:
        display(Markdown("**ì˜ˆì¸¡ vs ì •ë‹µ (Scatter)**"))
        figures["pred_vs_true"].show()

    if figures.get("residuals") is not None:
        display(Markdown("**ì”ì°¨ ì‚°ì ë„**"))
        figures["residuals"].show()

    if figures.get("learning_curve") is not None:
        display(Markdown("**Learning Curve**"))
        figures["learning_curve"].show()


def render_shap(runner: CatBoostRunner, topk: int = 20) -> None:
    """SHAP ì„¹ì…˜: ìš”ì•½ë°” + ê¸€ë¡œë²Œ ìˆœì„œ ì›Œí„°í´."""
    md_title("4) SHAP", level=3)

    # Top-k summary bar
    imp_df, imp_fig = runner.shap_summary_bar(topk=topk)
    display(Markdown(f"**SHAP Importance (Top {topk})**"))
    display(imp_df)
    if imp_fig is not None:
        imp_fig.show()

    # Waterfall (ì „ì—­ ì¤‘ìš”ë„ ìˆœì„œ ê³ ì •, idx=None â†’ ì¤‘ì•™ê°’ ì˜ˆì¸¡ ìƒ˜í”Œ)
    # v5ì˜ plot_waterfall_with_mean êµ¬í˜„ ì‚¬ìš©
    wf_df, wf_fig = runner.plot_waterfall_with_mean(idx=None, topk=topk, show=False)
    display(Markdown("**SHAP Waterfall (global-order, median-pred sample)**"))
    display(wf_df)
    wf_fig.show()


# =========================
# Main Orchestrator
# =========================
def build_and_render_report(deploy_dir: Path = DEPLOY_DIR,
                            exts: Tuple[str, ...] = MODEL_EXTS,
                            topk: int = TOPK) -> None:
    """models/deploy í•˜ë‹¨ ëª¨ë¸ë“¤ì„ ì¼ê´„ ë¡œë“œí•˜ê³ , ë…¸íŠ¸ë¶ì— ì„¹ì…˜ë³„ë¡œ ì¶œë ¥í•œë‹¤.

    íë¦„: í•™ìŠµíŒŒì¼(ë©”íƒ€) â†’ í…ŒìŠ¤íŠ¸ ì •ë³´ â†’ í•™ìŠµê²°ê³¼ â†’ SHAP
    """
    paths = find_models(deploy_dir, exts)
    meta_df = load_metadata_csv(deploy_dir)

    display(Markdown(f"## ğŸ“¦ Model Report â€” {len(paths)} artifact(s) in `{deploy_dir}`"))

    for p in paths:
        display(Markdown(f"---\n### ğŸ“ {p.name}"))
        # metadata join (filename í‚¤ ê¶Œì¥)
        meta_row = None
        if meta_df is not None:
            m = meta_df[meta_df["filename"].astype(str) == p.name]
            if len(m) > 0:
                meta_row = m.iloc[0]

        # 1) ë¡œë“œ & ì¸ìŠ¤í™íŠ¸
        out = load_and_inspect(str(p))
        runner: CatBoostRunner = out["runner"]
        bundle_summary: Dict[str, Any] = out["bundle_summary"]
        scores: Dict[str, float] = out.get("runner").scores()
        of_df = out.get("bundle_summary", {}).get("overfit_report")  # dictì¼ ìˆ˜ ìˆìŒ
        # dictì¸ ê²½ìš° DFë¡œ
        overfit_df = pd.DataFrame([of_df]) if isinstance(of_df, dict) else out.get("overfit_report_df", None)

        figures = out.get("figures", {})

        # 2) ì„¹ì…˜ ë Œë”ë§
        render_training_file_info(runner, bundle_summary, meta_row)
        render_test_info(runner, bundle_summary)
        render_training_results(runner, scores, overfit_df, figures)
        render_shap(runner, topk=topk)


# =========================
# Run (in notebook)
# =========================
# build_and_render_report(DEPLOY_DIR, MODEL_EXTS, TOPK)
